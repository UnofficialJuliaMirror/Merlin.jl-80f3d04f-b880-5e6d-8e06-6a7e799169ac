{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin\n is a deep learning framework written in \nJulia\n. It aims to provide a fast, flexible and compact deep learning library for machine learning.\n\n\nSee README.md for basic usage.\n\n\nBasically,\n\n\n\n\nWrap your data with \nVar\n type.\n\n\nApply functions to the \nVar\n.\n\n\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n10\n,\n3\n)\n\n\ny\n \n=\n \nf\n(\nx\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin  is a deep learning framework written in  Julia . It aims to provide a fast, flexible and compact deep learning library for machine learning.  See README.md for basic usage.  Basically,   Wrap your data with  Var  type.  Apply functions to the  Var .   x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Linear ( Float32 , 10 , 3 )  y   =   f ( x )", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/functions/", 
            "text": "Functions\n\n\nMerlin\n provides standard functions used in deep learning.\n\n\n\n\nIndex\n\n\n\n\nBase.:*\n\n\nBase.:+\n\n\nBase.:-\n\n\nBase.:.*\n\n\nBase.exp\n\n\nBase.getindex\n\n\nBase.log\n\n\nBase.max\n\n\nBase.reshape\n\n\nBase.sum\n\n\nBase.tanh\n\n\nBase.transpose\n\n\nBase.view\n\n\nMerlin.axsum\n\n\nMerlin.concat\n\n\nMerlin.logsoftmax\n\n\nMerlin.maxpooling\n\n\nMerlin.relu\n\n\nMerlin.sigmoid\n\n\nMerlin.softmax\n\n\n\n\n\n\nActivation Functions\n\n\n#\n\n\nMerlin.relu\n \n \nFunction\n.\n\n\nrelu(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.sigmoid\n \n \nFunction\n.\n\n\nsigmoid(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nFunction\n.\n\n\ntanh(x::Var)\n\n\n\n\n\nsource\n\n\n\n\nIndexing Functions\n\n\n#\n\n\nBase.getindex\n \n \nFunction\n.\n\n\ngetindex(x::Var, inds...)\n\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\ny\n \n=\n \nx\n[\n1\n:\n3\n]\n\n\ny\n \n=\n \nx\n[\n2\n]\n\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.view\n \n \nFunction\n.\n\n\nview(x::Var, inds...)\n\n\n\n\n\nsource\n\n\n\n\nManipulation Functions\n\n\n#\n\n\nMerlin.concat\n \n \nFunction\n.\n\n\nconcat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})\n\n\n\n\n\nConcatenate arrays along the given dimension.\n\n\nsource\n\n\n#\n\n\nBase.reshape\n \n \nFunction\n.\n\n\nreshape(x::Var, dims::Int...)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nFunction\n.\n\n\ntranspose(x::Var)\n\n\n\n\n\nsource\n\n\n\n\nMath Functions\n\n\n#\n\n\nBase.:+\n \n \nFunction\n.\n\n\n+(x1::Var, x2::Var)\n+(a::Number, x::Var)\n+(x::Var, a::Number)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.:-\n \n \nFunction\n.\n\n\n-(x1::Var, x2::Var)\n-(a::Number, x::Var)\n-(a::Number, x::Var)\n-(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.:*\n \n \nFunction\n.\n\n\n*(x1::Var, x2::Var)\n*(a::Number, x::Var)\n*(x::Var, a::Number)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.:.*\n \n \nFunction\n.\n\n\n.*(x1::Var, x2::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.axsum\n \n \nFunction\n.\n\n\naxsum\n\n\n\n\n\n\n\n\ny=sum_{i}aleft[i\night]cdot xleft[i\night]\n\n\n\n\n\nwhere $a[i]$ is a scalar and $x$ is scholar or vector. Every operation is broadcasted.\n\n\nsource\n\n\n#\n\n\nBase.exp\n \n \nFunction\n.\n\n\nexp\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.log\n \n \nFunction\n.\n\n\nlog\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.max\n \n \nFunction\n.\n\n\nmax(x::Var, dim::Int)\n\n\n\n\n\nCompute the maximum value along the given dimensions.\n\n\nsource\n\n\n#\n\n\nBase.sum\n \n \nFunction\n.\n\n\nsum(x, dim::Int)\n\n\n\n\n\nCompute the sum along the given dimensions.\n\n\nsource\n\n\n\n\nPooling Functions\n\n\n#\n\n\nMerlin.maxpooling\n \n \nFunction\n.\n\n\nmaxpooling(window, [stride, padding])\n\n\n\n\n\nArguments\n\n\n\n\nwindims::NTuple{N,Int}: window size\n\n\nstride::NTuple{N,Int}: stride size. Default: (1,1,...)\n\n\npaddims::NTuple{N,Int}: padding size. Default: (0,0,...)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n5\n,\n4\n,\n3\n,\n2\n))\n\n\ny\n \n=\n \nmaxpooling\n(\nx\n,\n \n(\n3\n,\n3\n),\n \nstride\n=\n(\n1\n,\n1\n),\n \npaddims\n=\n(\n0\n,\n0\n))\n\n\n\n\n\n\nsource\n\n\n\n\nSoftmax Functions\n\n\n#\n\n\nMerlin.logsoftmax\n \n \nFunction\n.\n\n\nlogsoftmax(x::Var, dim::Int)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.softmax\n \n \nFunction\n.\n\n\nsoftmax(x::Var, dim::Int)\n\n\n\n\n\nsource", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#functions", 
            "text": "Merlin  provides standard functions used in deep learning.", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#index", 
            "text": "Base.:*  Base.:+  Base.:-  Base.:.*  Base.exp  Base.getindex  Base.log  Base.max  Base.reshape  Base.sum  Base.tanh  Base.transpose  Base.view  Merlin.axsum  Merlin.concat  Merlin.logsoftmax  Merlin.maxpooling  Merlin.relu  Merlin.sigmoid  Merlin.softmax", 
            "title": "Index"
        }, 
        {
            "location": "/functions/#activation-functions", 
            "text": "#  Merlin.relu     Function .  relu(x::Var)  source  #  Merlin.sigmoid     Function .  sigmoid(x::Var)  source  #  Base.tanh     Function .  tanh(x::Var)  source", 
            "title": "Activation Functions"
        }, 
        {
            "location": "/functions/#indexing-functions", 
            "text": "#  Base.getindex     Function .  getindex(x::Var, inds...)  \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  y   =   x [ 1 : 3 ]  y   =   x [ 2 ]   source  #  Base.view     Function .  view(x::Var, inds...)  source", 
            "title": "Indexing Functions"
        }, 
        {
            "location": "/functions/#manipulation-functions", 
            "text": "#  Merlin.concat     Function .  concat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})  Concatenate arrays along the given dimension.  source  #  Base.reshape     Function .  reshape(x::Var, dims::Int...)  source  #  Base.transpose     Function .  transpose(x::Var)  source", 
            "title": "Manipulation Functions"
        }, 
        {
            "location": "/functions/#math-functions", 
            "text": "#  Base.:+     Function .  +(x1::Var, x2::Var)\n+(a::Number, x::Var)\n+(x::Var, a::Number)  source  #  Base.:-     Function .  -(x1::Var, x2::Var)\n-(a::Number, x::Var)\n-(a::Number, x::Var)\n-(x::Var)  source  #  Base.:*     Function .  *(x1::Var, x2::Var)\n*(a::Number, x::Var)\n*(x::Var, a::Number)  source  #  Base.:.*     Function .  .*(x1::Var, x2::Var)  source  #  Merlin.axsum     Function .  axsum   \ny=sum_{i}aleft[i\night]cdot xleft[i\night]   where $a[i]$ is a scalar and $x$ is scholar or vector. Every operation is broadcasted.  source  #  Base.exp     Function .  exp  source  #  Base.log     Function .  log  source  #  Base.max     Function .  max(x::Var, dim::Int)  Compute the maximum value along the given dimensions.  source  #  Base.sum     Function .  sum(x, dim::Int)  Compute the sum along the given dimensions.  source", 
            "title": "Math Functions"
        }, 
        {
            "location": "/functions/#pooling-functions", 
            "text": "#  Merlin.maxpooling     Function .  maxpooling(window, [stride, padding])  Arguments   windims::NTuple{N,Int}: window size  stride::NTuple{N,Int}: stride size. Default: (1,1,...)  paddims::NTuple{N,Int}: padding size. Default: (0,0,...)   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 5 , 4 , 3 , 2 ))  y   =   maxpooling ( x ,   ( 3 , 3 ),   stride = ( 1 , 1 ),   paddims = ( 0 , 0 ))   source", 
            "title": "Pooling Functions"
        }, 
        {
            "location": "/functions/#softmax-functions", 
            "text": "#  Merlin.logsoftmax     Function .  logsoftmax(x::Var, dim::Int)  source  #  Merlin.softmax     Function .  softmax(x::Var, dim::Int)  source", 
            "title": "Softmax Functions"
        }
    ]
}