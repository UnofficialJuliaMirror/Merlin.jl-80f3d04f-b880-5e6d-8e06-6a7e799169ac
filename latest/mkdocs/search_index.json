{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin\n is a deep learning framework written in \nJulia\n. It aims to provide a fast, flexible and compact deep learning library for machine learning.\n\n\nSee README.md for basic usage.\n\n\nMerlin\n provides some primitive functions (\nfunctor\ns). See \nFunctor\n for more information about each functors.", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin  is a deep learning framework written in  Julia . It aims to provide a fast, flexible and compact deep learning library for machine learning.  See README.md for basic usage.  Merlin  provides some primitive functions ( functor s). See  Functor  for more information about each functors.", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/functors/", 
            "text": "Functors\n\n\nFunctor\n is an abstract type of function object. Every \nFunctor\n implements forward and backward computation. The following are concrete types of \nFunctor\ns.\n\n\n#\n\n\nMerlin.Activation\n \n \nType\n.\n\n\nActivation function.\n\n\n\n\nActivation(mode::AbstractString)\n\n\n\n\n- mode: relu | tanh | sigmoid\n\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nActivation\n(\nrelu\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Concat\n \n \nType\n.\n\n\nConcat\n\n\nConcatenate arrays along the given dimension.\n\n\nFunctions\n\n\n\n\nConcat(dim::Int)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx1\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n7\n,\n5\n))\n\n\nx2\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\ny\n \n=\n \nConcat\n(\n1\n)(\nx1\n,\n \nx2\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.CrossEntropy\n \n \nType\n.\n\n\nCrossEntropy\n\n\nCompute cross-entropy between a true distribution: $p$ and the target distribution: $q_x$.\n\n\n$$\np\n$$\n\n\nand $q$ are assumed to be normalized (sums to one). To noamalize 'Var's, use \nLogSoftmax\n.\n\n\n$$\nf(x;p)=-\u2211\n{x} p log q\n\n$$\n\n\nFunctions\n\n\n\n\nCrossEntropy()\n\n\n\n\n\ud83d\udc49 Example\n\n\np\n \n=\n \nVar\n([\n1\n:\n5\n])\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nCrossEntropy\n()\n\n\ny\n \n=\n \nf\n(\np\n,\n \nq\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Linear\n \n \nType\n.\n\n\nLinear\n\n\nCompute linear transformation a.k.a. affine transformation.\n\n\n$$\nf(x) = W^{T}x + b\n$$\n\n\nwhere $W$ is a weight matrix, $b$ is a bias vector.\n\n\n\n\n\nArguments\n\n\n\n\nLinear(w,b)\n\n\nLinear{T}(::Type{T}, insize::Int, outsize::Int)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n \n10\n,\n \n3\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.LogSoftmax\n \n \nType\n.\n\n\nLogSoftmax\n\n\nCompute logarith of softmax function.\n\n\n$$\nf(x)=\frac{\u001bxp(x_{i})}{sum_{j}^{n}\u001bxp(x_{j})},;i=1,ldots,n\n$$\n\n\nFunctions\n\n\n\n\nLogSoftmax()\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nLogSoftmax\n()\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Lookup\n \n \nType\n.\n\n\nLookup\n\n\nLookup variables.\n\n\nFunctions\n\n\n\n\nLookup(insize::Int, outsize::Int)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\n1\n:\n1000\n,\n5\n,\n3\n))\n\n\nf\n \n=\n \nLookup\n(\nFloat32\n,\n1000\n,\n100\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Max\n \n \nType\n.\n\n\nMax\n\n\nCompute the maximum value of an array over the given dimensions.\n\n\nFunctions\n\n\n\n\nMax(dim::Int)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nMax\n(\n1\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Reshape\n \n \nType\n.\n\n\nReshape\n\n\nReshapes an array with the given dimensions.\n\n\nFunctions\n\n\n\n\nReshape(dims::Int...)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n,\n3\n))\n\n\nf\n \n=\n \nReshape\n(\n5\n,\n3\n,\n10\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.Softmax\n \n \nType\n.\n\n\nSoftmax\n\n\n$$\nf(x)=\frac{\u001bxp(x_{i})}{sum_{j}^{n}\u001bxp(x_{j})},;i=1,ldots,n\n$$\n\n\n$$\np(x) = {\\exp(f(x)) \\over \\sum_{x_2} \\exp(f(x))}\n$$\n\n\nFunctions\n\n\n\n\nSoftmax()\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nSoftmax\n()\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\nsource", 
            "title": "Functors"
        }, 
        {
            "location": "/functors/#functors", 
            "text": "Functor  is an abstract type of function object. Every  Functor  implements forward and backward computation. The following are concrete types of  Functor s.  #  Merlin.Activation     Type .  Activation function.   Activation(mode::AbstractString)   - mode: relu | tanh | sigmoid  \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Activation ( relu )  y   =   f ( x )   source  #  Merlin.Concat     Type .  Concat  Concatenate arrays along the given dimension.  Functions   Concat(dim::Int)   \ud83d\udc49 Example  x1   =   Var ( rand ( Float32 , 7 , 5 ))  x2   =   Var ( rand ( Float32 , 10 , 5 ))  y   =   Concat ( 1 )( x1 ,   x2 )   source  #  Merlin.CrossEntropy     Type .  CrossEntropy  Compute cross-entropy between a true distribution: $p$ and the target distribution: $q_x$.  $$\np\n$$  and $q$ are assumed to be normalized (sums to one). To noamalize 'Var's, use  LogSoftmax .  $$\nf(x;p)=-\u2211 {x} p log q \n$$  Functions   CrossEntropy()   \ud83d\udc49 Example  p   =   Var ([ 1 : 5 ])  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   CrossEntropy ()  y   =   f ( p ,   q )   source  #  Merlin.Linear     Type .  Linear  Compute linear transformation a.k.a. affine transformation.  $$\nf(x) = W^{T}x + b\n$$  where $W$ is a weight matrix, $b$ is a bias vector.   Arguments   Linear(w,b)  Linear{T}(::Type{T}, insize::Int, outsize::Int)   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Linear ( Float32 ,   10 ,   3 )  y   =   f ( x )   source  #  Merlin.LogSoftmax     Type .  LogSoftmax  Compute logarith of softmax function.  $$\nf(x)=\frac{\u001bxp(x_{i})}{sum_{j}^{n}\u001bxp(x_{j})},;i=1,ldots,n\n$$  Functions   LogSoftmax()   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   LogSoftmax ()  y   =   f ( x )   source  #  Merlin.Lookup     Type .  Lookup  Lookup variables.  Functions   Lookup(insize::Int, outsize::Int)   \ud83d\udc49 Example  x   =   Var ( rand ( 1 : 1000 , 5 , 3 ))  f   =   Lookup ( Float32 , 1000 , 100 )  y   =   f ( x )   source  #  Merlin.Max     Type .  Max  Compute the maximum value of an array over the given dimensions.  Functions   Max(dim::Int)   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Max ( 1 )  y   =   f ( x )   source  #  Merlin.Reshape     Type .  Reshape  Reshapes an array with the given dimensions.  Functions   Reshape(dims::Int...)   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 , 3 ))  f   =   Reshape ( 5 , 3 , 10 )  y   =   f ( x )   source  #  Merlin.Softmax     Type .  Softmax  $$\nf(x)=\frac{\u001bxp(x_{i})}{sum_{j}^{n}\u001bxp(x_{j})},;i=1,ldots,n\n$$  $$\np(x) = {\\exp(f(x)) \\over \\sum_{x_2} \\exp(f(x))}\n$$  Functions   Softmax()   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Softmax ()  y   =   f ( x )   source", 
            "title": "Functors"
        }, 
        {
            "location": "/network/", 
            "text": "#\n\n\nMerlin.Network\n \n \nType\n.\n\n\nNetwork\n\n\nNetwork\n is a container of \nFunctor\ns.\n\n\n\ud83d\udc49 Example\n\n\n\n\n\n\n\n\nsource", 
            "title": "Network"
        }, 
        {
            "location": "/optimizers/", 
            "text": "Optimizers\n\n\n#\n\n\nMerlin.SGD\n \n \nType\n.\n\n\nSGD\n\n\nComputes Stochastic Gradient Descent. After updated, gradient is set to be zero.\n\n\nFunctions\n\n\n\n\nSGD(rate::Float64)\n\n\n\n\n\ud83d\udc49 Example\n\n\nopt\n \n=\n \nSGD\n(\n0.001\n)\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n100\n,\n50\n)\n\n\n# compute gradient...\n\n\n\nupdate!\n(\nopt\n,\n \nf\n)\n \n# update parameters of `f`\n\n\nupdate!\n(\nopt\n,\n \nparam\n,\n \ngrad\n)\n \n# param -= rate * grad\n\n\n\n\n\n\nsource", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#optimizers", 
            "text": "#  Merlin.SGD     Type .  SGD  Computes Stochastic Gradient Descent. After updated, gradient is set to be zero.  Functions   SGD(rate::Float64)   \ud83d\udc49 Example  opt   =   SGD ( 0.001 )  f   =   Linear ( Float32 , 100 , 50 )  # compute gradient...  update! ( opt ,   f )   # update parameters of `f`  update! ( opt ,   param ,   grad )   # param -= rate * grad   source", 
            "title": "Optimizers"
        }
    ]
}