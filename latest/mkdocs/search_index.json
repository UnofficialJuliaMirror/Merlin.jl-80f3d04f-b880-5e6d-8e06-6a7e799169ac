{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin\n is a deep learning framework written in \nJulia\n. It aims to provide a fast, flexible and compact deep learning library for machine learning.\n\n\nSee README.md for basic usage.", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin  is a deep learning framework written in  Julia . It aims to provide a fast, flexible and compact deep learning library for machine learning.  See README.md for basic usage.", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/functions/", 
            "text": "Functions\n\n\n#\n\n\nMerlin.relu\n \n \nMethod\n.\n\n\nrelu(x)\n\n\n\n\n\n#\n\n\nMerlin.sigmoid\n \n \nMethod\n.\n\n\nsigmoid(x)\n\n\n\n\n\n#\n\n\nBase.tanh\n \n \nMethod\n.\n\n\ntanh(x)\n\n\n\n\n\n#\n\n\nMerlin.concat\n \n \nMethod\n.\n\n\nconcat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})\n\n\n\n\n\nConcatenate arrays along the given dimension.\n\n\n#\n\n\nMerlin.crossentropy\n \n \nMethod\n.\n\n\ncrossentropy(p,x)\n\n\n\n\n\nComputes cross-entropy between p and x. x is assumed to be unnormalized.\n\n\n\n\np: Vector{Int} or Matrix{Float}\n\n\n\n\n\ud83d\udc49 Example\n\n\np\n \n=\n \n[\n1\n:\n5\n;]\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\ny\n \n=\n \ncrossentropy\n(\np\n,\nx\n)\n\n\n\n\n\n\n#\n\n\nMerlin.gemm\n \n \nMethod\n.\n\n\ngemm(tA::Char, tB::Char, alpha::Float64, A::Var, B::Var)\ngemm(A::Var, B::Var)\n\n\n\n\n\nC = alpha * tA(A) * tB(B)\n\n\n\n\ntA: transpose ('T') or not ('N'). default: 'N'\n\n\ntB: same as tA\n\n\n\n\n#\n\n\nBase.max\n \n \nMethod\n.\n\n\n#\n\n\nBase.reshape\n \n \nMethod\n.\n\n\n#\n\n\nMerlin.softmax\n \n \nMethod\n.\n\n\nsoftmax(x::Var, dim::Int)\n\n\n\n\n\n#\n\n\nMerlin.logsoftmax\n \n \nMethod\n.\n\n\nlogsoftmax(x::Var, dim::Int)\n\n\n\n\n\nCompute log-softmax along the given axis.\n\n\n#\n\n\nBase.sum\n \n \nMethod\n.\n\n\n#\n\n\nBase.transpose\n \n \nMethod\n.\n\n\ntranspose(x::Var)", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#functions", 
            "text": "#  Merlin.relu     Method .  relu(x)  #  Merlin.sigmoid     Method .  sigmoid(x)  #  Base.tanh     Method .  tanh(x)  #  Merlin.concat     Method .  concat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})  Concatenate arrays along the given dimension.  #  Merlin.crossentropy     Method .  crossentropy(p,x)  Computes cross-entropy between p and x. x is assumed to be unnormalized.   p: Vector{Int} or Matrix{Float}   \ud83d\udc49 Example  p   =   [ 1 : 5 ;]  x   =   Var ( rand ( Float32 , 10 , 5 ))  y   =   crossentropy ( p , x )   #  Merlin.gemm     Method .  gemm(tA::Char, tB::Char, alpha::Float64, A::Var, B::Var)\ngemm(A::Var, B::Var)  C = alpha * tA(A) * tB(B)   tA: transpose ('T') or not ('N'). default: 'N'  tB: same as tA   #  Base.max     Method .  #  Base.reshape     Method .  #  Merlin.softmax     Method .  softmax(x::Var, dim::Int)  #  Merlin.logsoftmax     Method .  logsoftmax(x::Var, dim::Int)  Compute log-softmax along the given axis.  #  Base.sum     Method .  #  Base.transpose     Method .  transpose(x::Var)", 
            "title": "Functions"
        }
    ]
}