{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin\n is a deep learning framework written in \nJulia\n. It aims to provide a fast, flexible and compact deep learning library for machine learning.\n\n\nSee README.md for basic usage.", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin  is a deep learning framework written in  Julia . It aims to provide a fast, flexible and compact deep learning library for machine learning.  See README.md for basic usage.", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/functions/", 
            "text": "Functions\n\n\n\n\nActivation\n\n\n#\n\n\nMerlin.relu\n \n \nMethod\n.\n\n\nrelu(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.sigmoid\n \n \nMethod\n.\n\n\nsigmoid(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nMethod\n.\n\n\ntanh(x::Var)\n\n\n\n\n\nsource\n\n\nconcat(dim::Int, xs::Vector{Var})\ncrossentropy(p, x::Var)\ngemm(tA, tB, alpha, A::Var, B::Var)\nmax(x::Var, dim::Int)\nreshape(x::Var, dims)\nsoftmax(x::Var, dim::Int)\nlogsoftmax(x::Var, dim::Int)\nsum(x, dim::Int)\ntranspose(x::Var)", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#functions", 
            "text": "", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#activation", 
            "text": "#  Merlin.relu     Method .  relu(x::Var)  source  #  Merlin.sigmoid     Method .  sigmoid(x::Var)  source  #  Base.tanh     Method .  tanh(x::Var)  source  concat(dim::Int, xs::Vector{Var})\ncrossentropy(p, x::Var)\ngemm(tA, tB, alpha, A::Var, B::Var)\nmax(x::Var, dim::Int)\nreshape(x::Var, dims)\nsoftmax(x::Var, dim::Int)\nlogsoftmax(x::Var, dim::Int)\nsum(x, dim::Int)\ntranspose(x::Var)", 
            "title": "Activation"
        }
    ]
}