{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin.jl\n is a neural network library in \nJulia\n.\n\n\n\n\nInstall\n\n\n#julia\n Pkg.clone(\nhttps://github.com/hshindo/Merlin.jl.git\n)\n\n\n\n\n\n\n\n\nRequirements\n\n\n\n\nOptional\n\n\n\n\ncuDNN\n v4 (for CUDA GPU)", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin.jl  is a neural network library in  Julia .", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/#install", 
            "text": "#julia  Pkg.clone( https://github.com/hshindo/Merlin.jl.git )", 
            "title": "Install"
        }, 
        {
            "location": "/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/#optional", 
            "text": "cuDNN  v4 (for CUDA GPU)", 
            "title": "Optional"
        }, 
        {
            "location": "/overview/", 
            "text": "Overview\n\n\nMerlin.jl provides many primitive functions.\n\n\n\n\nDecoding\n\n\n\n\nCreate \nVariable\n from \nArray\n (CPU) or \nCudaArray\n (CUDA GPU).\n\n\nCreate \nFunctor\ns.\n\n\nApply the functors to the variable.\n\n\n\n\nusing\n \nMerlin\n\n\n\nx\n \n=\n \nVariable\n(\nrand\n(\nFloat32\n,\n50\n,\n5\n))\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n50\n,\n30\n)\n\n\ny\n \n=\n \nf\n(\nx\n)\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\nCreate \nOptimizer\n.\n\n\nDecode your variables.\n\n\nCompute gradient.\n\n\nUpdate \nFunctor\ns with your \nOptimizer\n.\n\n\n\n\nusing\n \nMerlin\n\n\n\nopt\n \n=\n \nSGD\n(\n0.001\n)\n\n\nf\n \n=\n \n[\nLinear\n(\nFloat32\n,\n50\n,\n30\n),\n \nReLU\n(),\n \nLinear\n(\nFloat32\n,\n30\n,\n10\n)]\n\n\n\nfor\n \ni\n \n=\n \n1\n:\n10\n\n  \nx\n \n=\n \nVariable\n(\nrand\n(\nFloat32\n,\n50\n,\n20\n))\n\n  \ny\n \n=\n \nf\n(\nx\n)\n \n|\n \nCrossEntropy\n(\n...\n)\n\n  \ngradient!\n(\ny\n)\n\n  \nupdate!\n(\nopt\n,\n \nf\n)\n\n\nend", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#overview", 
            "text": "Merlin.jl provides many primitive functions.", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#decoding", 
            "text": "Create  Variable  from  Array  (CPU) or  CudaArray  (CUDA GPU).  Create  Functor s.  Apply the functors to the variable.   using   Merlin  x   =   Variable ( rand ( Float32 , 50 , 5 ))  f   =   Linear ( Float32 , 50 , 30 )  y   =   f ( x )", 
            "title": "Decoding"
        }, 
        {
            "location": "/overview/#training", 
            "text": "Create  Optimizer .  Decode your variables.  Compute gradient.  Update  Functor s with your  Optimizer .   using   Merlin  opt   =   SGD ( 0.001 )  f   =   [ Linear ( Float32 , 50 , 30 ),   ReLU (),   Linear ( Float32 , 30 , 10 )]  for   i   =   1 : 10 \n   x   =   Variable ( rand ( Float32 , 50 , 20 )) \n   y   =   f ( x )   |   CrossEntropy ( ... ) \n   gradient! ( y ) \n   update! ( opt ,   f )  end", 
            "title": "Training"
        }, 
        {
            "location": "/types/", 
            "text": "Types\n\n\nThere are three basic types:\n\n\n\n\nVariable\n\n\nFunctor\n\n\nOptimizer\n\n\n\n\n\n\nVariable\n\n\nVariable\n has \nvalue\n and \ngrad\n.\n\n\n#x = Variable(AFArray(Float32,10,5))\n\n\n#x.value\n\n\n#x.grad\n\n\n\n\n\n\n\n\nFunctor\n\n\nFunctor\n is an abstract type of functors.", 
            "title": "Types"
        }, 
        {
            "location": "/types/#types", 
            "text": "There are three basic types:   Variable  Functor  Optimizer", 
            "title": "Types"
        }, 
        {
            "location": "/types/#variable", 
            "text": "Variable  has  value  and  grad .  #x = Variable(AFArray(Float32,10,5))  #x.value  #x.grad", 
            "title": "Variable"
        }, 
        {
            "location": "/types/#functor", 
            "text": "Functor  is an abstract type of functors.", 
            "title": "Functor"
        }, 
        {
            "location": "/functors/", 
            "text": "Functors\n\n\n{docs}\nAdd\nConcat\nCrossEntropy\nLinear\nLogSoftmax\nLookup\nMax\nMultiply\nReLU\nReshape\nSigmoid\nSoftmax\nSubtract\nTanh\nWindow2D", 
            "title": "Functors"
        }, 
        {
            "location": "/functors/#functors", 
            "text": "{docs}\nAdd\nConcat\nCrossEntropy\nLinear\nLogSoftmax\nLookup\nMax\nMultiply\nReLU\nReshape\nSigmoid\nSoftmax\nSubtract\nTanh\nWindow2D", 
            "title": "Functors"
        }, 
        {
            "location": "/optimizers/", 
            "text": "Optimizers\n\n\n#\n\n\nMerlin.SGD\n \n \nType\n.\n\n\n\n\nSGD\n\n\nComputes Stochastic Gradient Descent. After updated, gradient is set to be zero.\n\n\nFunctions\n\n\n\n\nSGD(rate::Float64)\n\n\n\n\n\ud83d\udc49 Example\n\n\nopt\n \n=\n \nSGD\n(\n0.001\n)\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n100\n,\n50\n)\n\n\n# compute gradient...\n\n\n\nupdate!\n(\nopt\n,\n \nf\n)\n \n# update parameters of `f`\n\n\nupdate!\n(\nopt\n,\n \nparam\n,\n \ngrad\n)\n \n# param -= rate * grad", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#optimizers", 
            "text": "#  Merlin.SGD     Type .", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#sgd", 
            "text": "Computes Stochastic Gradient Descent. After updated, gradient is set to be zero.", 
            "title": "SGD"
        }, 
        {
            "location": "/optimizers/#functions", 
            "text": "SGD(rate::Float64)", 
            "title": "Functions"
        }, 
        {
            "location": "/optimizers/#example", 
            "text": "opt   =   SGD ( 0.001 )  f   =   Linear ( Float32 , 100 , 50 )  # compute gradient...  update! ( opt ,   f )   # update parameters of `f`  update! ( opt ,   param ,   grad )   # param -= rate * grad", 
            "title": "\ud83d\udc49 Example"
        }
    ]
}