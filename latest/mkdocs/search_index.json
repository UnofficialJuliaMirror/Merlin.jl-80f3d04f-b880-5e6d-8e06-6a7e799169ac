{
    "docs": [
        {
            "location": "/", 
            "text": "Merlin.jl\n\n\nMerlin\n is a deep learning framework written in \nJulia\n. It aims to provide a fast, flexible and compact deep learning library for machine learning.\n\n\nSee README.md for basic usage.\n\n\nBasically,\n\n\n\n\nWrap your data with \nVar\n type.\n\n\nApply functions to the \nVar\n.\n\n\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n10\n,\n5\n))\n\n\nf\n \n=\n \nLinear\n(\nFloat32\n,\n10\n,\n3\n)\n\n\ny\n \n=\n \nf\n(\nx\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#merlinjl", 
            "text": "Merlin  is a deep learning framework written in  Julia . It aims to provide a fast, flexible and compact deep learning library for machine learning.  See README.md for basic usage.  Basically,   Wrap your data with  Var  type.  Apply functions to the  Var .   x   =   Var ( rand ( Float32 , 10 , 5 ))  f   =   Linear ( Float32 , 10 , 3 )  y   =   f ( x )", 
            "title": "Merlin.jl"
        }, 
        {
            "location": "/functions/", 
            "text": "Functions\n\n\nMerlin\n provides standard functions used in deep learning.\n\n\n\n\nIndex\n\n\n\n\nBase.getindex\n\n\nBase.reshape\n\n\nBase.tanh\n\n\nBase.transpose\n\n\nMerlin.concat\n\n\nMerlin.logsoftmax\n\n\nMerlin.maxpooling\n\n\nMerlin.relu\n\n\nMerlin.sigmoid\n\n\nMerlin.softmax\n\n\n\n\n\n\nActivation\n\n\n#\n\n\nMerlin.relu\n \n \nFunction\n.\n\n\nrelu(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.sigmoid\n \n \nFunction\n.\n\n\nsigmoid(x::Var)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.tanh\n \n \nFunction\n.\n\n\ntanh(x::Var)\n\n\n\n\n\nsource\n\n\n\n\nIndexing\n\n\ngetindex\nview\n\n\n\n\n\n\n\nManipulation\n\n\n#\n\n\nMerlin.concat\n \n \nFunction\n.\n\n\nconcat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})\n\n\n\n\n\nConcatenate arrays along the given dimension.\n\n\nsource\n\n\n#\n\n\nBase.reshape\n \n \nFunction\n.\n\n\nreshape(x::Var, dims::Int...)\n\n\n\n\n\nsource\n\n\n#\n\n\nBase.transpose\n \n \nFunction\n.\n\n\ntranspose(x::Var)\n\n\n\n\n\nsource\n\n\n\n\nMath\n\n\nadd\nexp\nlog\nmultiply\n\n\n\n\n\n\n\nPooling\n\n\n#\n\n\nMerlin.maxpooling\n \n \nFunction\n.\n\n\nmaxpooling(window, [stride, padding])\n\n\n\n\n\nArguments\n\n\n\n\nwindims::NTuple{N,Int}: window size\n\n\nstride::NTuple{N,Int}: stride size. Default: (1,1,...)\n\n\npaddims::NTuple{N,Int}: padding size. Default: (0,0,...)\n\n\n\n\n\ud83d\udc49 Example\n\n\nx\n \n=\n \nVar\n(\nrand\n(\nFloat32\n,\n5\n,\n4\n,\n3\n,\n2\n))\n\n\ny\n \n=\n \nmaxpooling\n(\nx\n,\n \n(\n3\n,\n3\n),\n \nstride\n=\n(\n1\n,\n1\n),\n \npaddims\n=\n(\n0\n,\n0\n))\n\n\n\n\n\n\nsource\n\n\n\n\nSoftmax\n\n\n#\n\n\nMerlin.logsoftmax\n \n \nFunction\n.\n\n\nlogsoftmax(x::Var, dim::Int)\n\n\n\n\n\nsource\n\n\n#\n\n\nMerlin.softmax\n \n \nFunction\n.\n\n\nsoftmax(x::Var, dim::Int)\n\n\n\n\n\nsource", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#functions", 
            "text": "Merlin  provides standard functions used in deep learning.", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#index", 
            "text": "Base.getindex  Base.reshape  Base.tanh  Base.transpose  Merlin.concat  Merlin.logsoftmax  Merlin.maxpooling  Merlin.relu  Merlin.sigmoid  Merlin.softmax", 
            "title": "Index"
        }, 
        {
            "location": "/functions/#activation", 
            "text": "#  Merlin.relu     Function .  relu(x::Var)  source  #  Merlin.sigmoid     Function .  sigmoid(x::Var)  source  #  Base.tanh     Function .  tanh(x::Var)  source", 
            "title": "Activation"
        }, 
        {
            "location": "/functions/#indexing", 
            "text": "getindex\nview", 
            "title": "Indexing"
        }, 
        {
            "location": "/functions/#manipulation", 
            "text": "#  Merlin.concat     Function .  concat(dim::Int, xs::Var...)\nconcat(dim::Int, xs::Vector{Var})  Concatenate arrays along the given dimension.  source  #  Base.reshape     Function .  reshape(x::Var, dims::Int...)  source  #  Base.transpose     Function .  transpose(x::Var)  source", 
            "title": "Manipulation"
        }, 
        {
            "location": "/functions/#math", 
            "text": "add\nexp\nlog\nmultiply", 
            "title": "Math"
        }, 
        {
            "location": "/functions/#pooling", 
            "text": "#  Merlin.maxpooling     Function .  maxpooling(window, [stride, padding])  Arguments   windims::NTuple{N,Int}: window size  stride::NTuple{N,Int}: stride size. Default: (1,1,...)  paddims::NTuple{N,Int}: padding size. Default: (0,0,...)   \ud83d\udc49 Example  x   =   Var ( rand ( Float32 , 5 , 4 , 3 , 2 ))  y   =   maxpooling ( x ,   ( 3 , 3 ),   stride = ( 1 , 1 ),   paddims = ( 0 , 0 ))   source", 
            "title": "Pooling"
        }, 
        {
            "location": "/functions/#softmax", 
            "text": "#  Merlin.logsoftmax     Function .  logsoftmax(x::Var, dim::Int)  source  #  Merlin.softmax     Function .  softmax(x::Var, dim::Int)  source", 
            "title": "Softmax"
        }
    ]
}